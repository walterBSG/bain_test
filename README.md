
# Bain Test Real Estate Model

This repository contains the code for a property valuation model for residential properties in Chile. The project includes a training pipeline and an API for making predictions.

## Project Structure
```
bain_test/
│
├── api/
│   ├── __init__.py
│   ├── main.py
│   ├── routes.py
│   ├── controller.py
│   ├── models.py
│   └── requirements.txt
│
├── data/
│   ├── train.csv
│   ├── test.csv
│
├── model/
│   ├── pipeline.py
│   ├── train_model.py
│   ├── evaluate_model.py
│   └── model.joblib
│
├── utils/
│   ├── data_loader.py
│   └── logger.py
│
├── tests/
│   └── test_pipeline.py
│
├── Dockerfile
├── .gitignore
├── README.md
└── requirements.txt
```

## Setup and Installation

### Prerequisites
- Docker
- Python 3.9

### Building the Docker Image
1. Navigate to the project root directory.
2. Build the Docker image:
   ```bash
   docker build -t bain_test .
   ```

### Running the Training Pipeline
1. Run the Docker container with a command to execute the training script:
   ```bash
   docker run --rm -v $(PWD)/model:/app/model bain_test python model/train_model.py
   ```
   or to use a DataBank
   ```bash
   docker run --rm -v $(PWD)/model:/app/model bain_test python model/train_model.py --from_db=True
   ```

### Running the Evaluation Script
1. Run the Docker container with a command to execute the evaluation script:
   ```bash
   docker run --rm -v $(PWD)/model:/app/model bain_test python model/evaluate_model.py
   ```
   or to use a DataBank
   ```bash
   docker run --rm -v $(PWD)/model:/app/model bain_test python model/evaluate_model.py --from_db=True
   ```

### Running the API
1. Run the Docker container to start the API:
   ```bash
   docker run -p 8000:8000 bain_test
   ```

### Using the API
1. Open your web browser and navigate to `http://localhost:8000/docs` to access the Swagger UI for the API.
2. To use the API, make a POST request to `/predict` with property features. Example request:
   ```json
   {
     "type": "string",
     "sector": "string",
     "net_usable_area": 0,
     "net_area": 0,
     "n_rooms": 0,
     "n_bathroom": 0,
     "latitude": 0,
     "longitude": 0
   }
   ```

### API Security
The API uses an API key for security. You need to provide the API key in your requests. The API key and its name are loaded from the `api_keys.env` file.

### Example cURL Request
To make a request with the API key using `curl`:
```bash
curl -X POST "http://localhost:8000/predict" \
-H "accept: application/json" \
-H "Content-Type: application/json" \
-H "access_token: mysecureapikey" \
-d '{
  "type": "string",
  "sector": "string",
  "net_usable_area": 0,
  "net_area": 0,
  "n_rooms": 0,
  "n_bathroom": 0,
  "latitude": 0,
  "longitude": 0
}'
```

## Dependencies
- Dependencies can be found in the `requirements.txt` file

## Assumptions
- The environment variables `API_KEY`, `API_KEY_NAME` and `DATABASE_URL` for API keys are stored in `api_keys.env`.
- The model file `model.joblib` is generated by the training script and used by the API.
- The client have a Postgre DB in similar configuration to the .csv files

## Areas of Improvement
- Implement a Database: Integrate a robust database (e.g., PostgreSQL, MySQL) for efficient and secure data storage, management, and retrieval.
- Log and Store Inputs: Log every API request and store the inputs in the database for future analysis, monitoring, and model improvement.
- Enhance Logging and Error Handling: Implement detailed logging and comprehensive error handling to track system performance and diagnose issues effectively.
- Strengthen API Security: Implement additional security measures, such as OAuth2 to protect the API from unauthorized access.
- Expand API Functionality: Extend the API to include features such as model training, evaluation, data input, data retrieval, historical data analysis, and more. This will make the system more versatile and user-friendly.
- Feature Engineering: Create new features (e.g., price_per_square_meter, age_of_property) and interaction terms. Use feature importance techniques to retain impactful features.
- Hyperparameter Tuning: Optimize model parameters using Grid Search, Random Search, or Bayesian Optimization (e.g., Optuna).
- Cross-Validation: Use k-fold cross-validation to ensure the model generalizes well to unseen data.
- Algorithm Selection and Ensembling: Experiment with algorithms like Random Forest, XGBoost, and LightGBM. Use ensembling techniques (e.g., stacking, bagging, boosting) to improve performance.
